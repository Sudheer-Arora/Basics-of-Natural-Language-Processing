{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOz1bPJUF2xhd355+8FP7EO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sudheer-Arora/Basics-of-Natural-Language-Processing/blob/main/Basics_of_Natural_Language_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SasNgVa7sqME",
        "outputId": "7999a302-b4cf-4093-8f2a-970151b47dbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello world this is a sample text for nlp preprocessing\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = text.lower()\n",
        "    return text\n",
        "sample_text = \"Hello World! This is a sample text for NLP preprocessing\"\n",
        "processed_text = preprocess_text(sample_text)\n",
        "print(processed_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import nltk"
      ],
      "metadata": {
        "id": "ENpoEGHStXKx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dg54L03jtguO",
        "outputId": "b801bec2-a43f-446d-9443-11203599f504"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = word_tokenize(processed_text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGrSgoH_tnkL",
        "outputId": "909e3df2-806a-4cfd-b983-2788f87d9b92"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', 'world', 'this', 'is', 'a', 'sample', 'text', 'for', 'nlp', 'preprocessing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uE_0ipv1uXLQ",
        "outputId": "d56a3b75-e686-4af2-8a69-a32097f5ae59"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', 'world', 'sample', 'text', 'nlp', 'preprocessing']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMgXpFCruina",
        "outputId": "4d204e5a-c9be-4b06-b494-9c179e6760ae"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'but', 'over', \"she's\", 'where', 'ourselves', 'they', 're', \"won't\", 'his', 'below', 'too', \"shouldn't\", 'hers', 'herself', \"don't\", 'why', 'own', 'are', 'most', 'same', 'at', 'ain', 'than', 'what', 'so', 'her', 'has', 'not', 'weren', 'here', 'if', \"hasn't\", 'she', 'ma', 'between', 'nor', 'because', 'on', 'we', 'him', 'now', 'yours', 'being', 'after', 'in', 'those', 'some', 's', 'be', 'theirs', \"doesn't\", 'for', 'before', 'aren', \"aren't\", 'your', 'is', 'when', 'm', 'down', 'or', 'which', 'such', 'each', 'through', 'does', 'any', 'who', 'don', 'can', 'during', 'won', 'under', 'doing', 'yourselves', 'their', 'i', 'few', 'isn', 'them', 'other', 'again', 'of', 'itself', 'needn', 'an', 'should', 'do', 'once', 'further', \"wouldn't\", 'how', 'the', 'hadn', \"haven't\", 'from', \"that'll\", 'o', 'with', \"you're\", \"you'd\", 'this', 'and', 'while', 'having', 'mustn', 'yourself', 'himself', 'it', 'there', 'shan', \"weren't\", \"hadn't\", 'up', 'above', 'wasn', 'were', 'then', 'haven', 'themselves', 'off', 'very', 'y', 'only', 'will', 'couldn', 'as', 'didn', 'out', 'you', 'just', 'had', \"it's\", 'mightn', \"didn't\", 'a', \"you've\", \"should've\", \"mightn't\", 't', 'doesn', 'that', 'have', 'my', 'its', 'am', 'against', 'by', 'shouldn', 'until', \"couldn't\", 'our', 'whom', 'me', 'he', 'ours', 'more', 'd', \"shan't\", 'hasn', 'myself', 'both', 'll', \"isn't\", 'been', 'these', 'about', 'into', \"you'll\", \"wasn't\", \"needn't\", 'did', 'wouldn', 'was', \"mustn't\", 've', 'no', 'all', 'to'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
        "print(stemmed_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6wIEPntuvx9",
        "outputId": "9a2a933e-f1ff-401a-8dde-488e788364ae"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', 'world', 'sampl', 'text', 'nlp', 'preprocess']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import spacy\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "text = \"The boy was going for a trip where he could say that he hiked, danced, sung, swam, surfed and cooked.\"\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "filtered_tokens = [\"running\", \"jumps\", \"easily\", \"fairly\"]\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet.zip')\n",
        "    nltk.data.find('corpora/omw-1.4.zip')\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "    print(\"Lemmatized tokens using NLTK:\", lemmatized_tokens)\n",
        "except LookupError:\n",
        "    print(\"WordNet resource not found. Please ensure it is downloaded properly.\")\n",
        "doc = nlp(text)\n",
        "spacy_lemmatized_tokens = [token.lemma_ for token in doc]\n",
        "print(\"Lemmatized tokens using spaCy:\", spacy_lemmatized_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad_PttJCvtqC",
        "outputId": "381b784a-68ba-4d04-9436-c0b0cc210cae"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized tokens using NLTK: ['running', 'jump', 'easily', 'fairly']\n",
            "Lemmatized tokens using spaCy: ['the', 'boy', 'be', 'go', 'for', 'a', 'trip', 'where', 'he', 'could', 'say', 'that', 'he', 'hike', ',', 'danced', ',', 'sung', ',', 'swam', ',', 'surfed', 'and', 'cook', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "print(lemmatized_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mc21a8lMwJmY",
        "outputId": "0d036bc6-b758-4a57-d6fa-19bdf53cbbed"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['running', 'jump', 'easily', 'fairly']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "texts = [\"I love programming.\", \"Python is awesome.\", \"I hate bugs.\", \"Debugging is fun.\"]\n",
        "labels = [1, 1, 0, 1]\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "vectorized_texts = tfidf_vectorizer.fit_transform(texts)\n",
        "print(vectorized_texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWkG4OmQw0iW",
        "outputId": "e6643a23-dbbf-451b-bd36-96ecb202dbc9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 7)\t0.7071067811865476\n",
            "  (0, 6)\t0.7071067811865476\n",
            "  (1, 0)\t0.6176143709756019\n",
            "  (1, 5)\t0.48693426407352264\n",
            "  (1, 8)\t0.6176143709756019\n",
            "  (2, 1)\t0.7071067811865476\n",
            "  (2, 4)\t0.7071067811865476\n",
            "  (3, 3)\t0.6176143709756019\n",
            "  (3, 2)\t0.6176143709756019\n",
            "  (3, 5)\t0.48693426407352264\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(vectorized_texts, labels, test_size=0.25, random_state=42)\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzrzvHSMxDqb",
        "outputId": "5ca1c8a5-efcf-4d85-fc00-3ef16a5db92d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 100.00%\n"
          ]
        }
      ]
    }
  ]
}